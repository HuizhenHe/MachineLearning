# MachineLearning
## 基础理论
**1、机器学习是建立数学模型；**    
**2、机器学习实际是求解最优化问题——求解结构风险函数最小化的问题。**   
>譬如线性回归问题中，预期是拟合一条直线使得该直线预测出的值与实际的值尽可能的小（该差值又称残差/损失函数）。然而在实际情况下，仅考虑损失函数最小化会导致模型过拟合，故需要加入正则化函数以平衡，加入了**正则化函数的损失函数称之为结构风险函数**。即最终机器学习实际是求解结构风险函数最小化问题。 
>>正则化函数、损失函数存在多个，根据不同的机器学习进行选择  
>>
>Q1：所谓的求解最优化问题在sklearn上具体是如何呈现的？  
>
**针对求解结构风险函数进行算法优化：①梯度下降法；②牛顿法；③拟牛顿法**    
>**梯度下降法**    
>>>*已知求解最小值的方式可以通过f'(x)=0进行求解，但当f(x)式子过于复杂时，一阶求导后仍然求解困难，此时引进：梯度下降法*
>>>
>>1、梯度下降法是种基于一阶泰勒展开式求最小值的方法，其思想是随机选择函数上的一个点按照一定的步长不断地进行梯度下降，当|f(x_k+1)-f(x_k)|<ε停止迭代;   
2、梯度下降法的实质是通过牺牲一定的算法精度以换取计算的便利;   
3、影响因子：初始值；步长；        
4、缺点：容易陷入局部最优。
>>
>**改进：随机梯度下降法**    
>>两个改进点：固定步长改为随机步长；随机抽取样本。    
理解：随机梯度下降法以损失很小的一部分精确度和增加一定数量的迭代次数为代价，保证了迭代结果的有效性。
>>
>**牛顿法**   
>>牛顿法是种基于二阶泰勒展开式求最小值的方法，由于使用了二阶求导，一定程度地把曲线化为了直线，故直接采取f'(x)=0来求最小值。   
>>
>**拟牛顿法**
>>拟牛顿法是种在牛顿法计算f'(x)=0过程中改变其某些矩阵来近似达到牛顿法的方法。
>>
## 线性回归模型
**线性回归（LinearRegression）**

